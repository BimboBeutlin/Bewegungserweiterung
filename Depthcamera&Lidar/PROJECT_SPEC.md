# Projekt: Adaptive HÃ¶henanpassung zur HÃ¶hlenerkundung
## MPEC - Motion Path Extension and Control

> **Projektbeschreibung**: Entwicklung einer autonomen HÃ¶henanpassungssoftware fÃ¼r den Unitree GO2 Roboterhund zur Erkundung enger HÃ¶hleneingÃ¤nge und niedriger DurchgÃ¤nge

---

## ðŸ“‹ ProjektÃ¼bersicht

### Zielsetzung
Entwicklung einer intelligenten Steuerungssoftware fÃ¼r den Unitree GO2, die:
- **HÃ¶hleneingÃ¤nge/enge DurchgÃ¤nge automatisch erkennt** (HÃ¶he, Breite, Neigung)
- **Optimale KÃ¶rperhaltung berechnet** (Normal â†’ Niedrig â†’ Liegend)
- **Sicheres Ducken/Senken ermÃ¶glicht** (mit Sicherheitsabstand)
- **Multi-Sensor-Fusion nutzt** (Stereo-Depth-Kamera + LiDAR)
- **Autonome HÃ¶hlenanpassung** in Echtzeit durchfÃ¼hrt

### AnwendungsfÃ¤lle
- **HÃ¶hlenerkundung**: Autonome Navigation durch enge HÃ¶hlengÃ¤nge
- **Search & Rescue**: Durchqueren von TrÃ¼mmern und engen RÃ¤umen
- **Inspection**: Inspektion unter Fahrzeugen, in Rohren, niedrigen GebÃ¤uden
- **Research**: Geologische und speleologische Forschung

---

## ðŸŽ¯ Projektziele

### Hauptziele
1. **ZuverlÃ¤ssige Eingangsgeometrie-Erkennung** mit >90% Genauigkeit
2. **Echtzeit-Verarbeitung** (<100ms pro Frame)
3. **Robuste Multi-Sensor-Fusion** (Stereo-Depth-Kamera + LiDAR)
4. **PrÃ¤zise HÃ¶henberechnung** mit cm-Genauigkeit
5. **Sichere autonome KÃ¶rperanpassung** des Unitree GO2

### Technische Anforderungen (MVP)
- **Erkennungsreichweite**: 0,5m - 3,0m
- **Minimale Eingangsbreite**: 0,4m (Roboterbreite + Sicherheit)
- **Erkennbare EingangshÃ¶he**: 0,2m - 1,0m
- **Verarbeitungsrate**: mindestens 10 FPS
- **Sicherheitsabstand**: 5-10cm Ã¼ber der DurchgangshÃ¶he
- **Reaktionszeit**: <500ms von Erkennung bis Anpassung

### Roboter-Spezifikationen (Unitree GO2)
- **Normale KÃ¶rperhÃ¶he**: ~30cm
- **Minimale KÃ¶rperhÃ¶he**: ~10-15cm (geduckter Stand)
- **Liegende Position**: ~8cm
- **Breite**: ~28cm
- **LÃ¤nge**: ~65cm
- **Max. Geschwindigkeit**: 3,5 m/s

---

## ðŸ”§ Hardware-Setup

### Sensoren

#### 1. Intel RealSense Tiefenkamera (D400-Serie)
- **Modell**: D435/D455 empfohlen
- **AuflÃ¶sung**: 640x480 @ 30fps (Depth + Color)
- **Reichweite**: 0,3m - 10m
- **FOV**: 87Â° Ã— 58Â° (diagonal ~95Â°)
- **Technologie**: Stereo-Vision
- **ZusÃ¤tzlich**: Infrarot-Projektor fÃ¼r Low-Light

**Spezifikationen:**
```
Depth Stream:  640x480, z16 Format, 30 FPS
Color Stream:  640x480, BGR8 Format, 30 FPS
Infrared:      640x480, Y8 Format, 30 FPS
Depth Range:   0.3m - 3m (optimal), bis 10m mÃ¶glich
Accuracy:      <2% bei 2m Entfernung
```

#### 2. LiDAR-Sensor (Optional)
- **Empfohlen**: RPLidar A1/A2 oder SLAMTEC
- **Typ**: 2D 360Â° Scanner
- **Reichweite**: 6m - 12m
- **Scan-Rate**: 5-10 Hz
- **AuflÃ¶sung**: 360 Messpunkte pro Scan

**Spezifikationen:**
```
Scan Rate:     5.5 Hz (A1), 10 Hz (A2)
Sample Rate:   8000 samples/sec
Range:         12m (A1), 16m (A2)
Accuracy:      Â±0.5cm
Angular:       0.9Â° - 1Â° AuflÃ¶sung
```

### Systemanforderungen
- **OS**: Ubuntu 20.04/22.04 oder macOS 10.14+
- **CPU**: Intel i5 oder besser (i7 empfohlen)
- **RAM**: mindestens 4GB (8GB empfohlen)
- **USB**: USB 3.0 Port fÃ¼r RealSense
- **Python**: 3.7+

---

## ðŸ—ï¸ Software-Architektur

### Systemkomponenten (UML-basiert)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HAUPTSTEUERUNG (Main Loop)                   â”‚
â”‚                  (AdaptiveHeightController)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  SENSING MODUL    â”‚              â”‚   UNITREE SDK      â”‚
    â”‚                   â”‚              â”‚   Interface        â”‚
    â”‚ â€¢ Stereo-Depth    â”‚              â”‚                    â”‚
    â”‚ â€¢ LiDAR 2D        â”‚              â”‚ â€¢ Body Height      â”‚
    â”‚ â€¢ IMU/Pose        â”‚              â”‚ â€¢ Posture Mode     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â€¢ Emergency Stop   â”‚
             â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
    â”‚  PROCESSING       â”‚                        â”‚
    â”‚  (Black Box)      â”‚                        â”‚
    â”‚                   â”‚                        â”‚
    â”‚ â€¢ Geometrie-      â”‚                        â”‚
    â”‚   Extraktion      â”‚                        â”‚
    â”‚ â€¢ HÃ¶hen-Berechnungâ”‚                        â”‚
    â”‚ â€¢ Safety-Check    â”‚                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
             â”‚                                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
    â”‚  DECISION         â”‚                        â”‚
    â”‚  (State Machine)  â”‚                        â”‚
    â”‚                   â”‚                        â”‚
    â”‚ â€¢ Passt so        â”‚â”€â”€â”€â”€â”€â–º WEITER          â”‚
    â”‚ â€¢ Zu hoch         â”‚â”€â”€â”€â”€â”€â–º DUCKEN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ Viel zu hoch    â”‚â”€â”€â”€â”€â”€â–º HINLEGEN â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ Blockiert       â”‚â”€â”€â”€â”€â”€â–º STOPP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
                                                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ACTUATION (Motor Control)                             â”‚
    â”‚  â€¢ Normal (h=30cm) â€¢ Niedrig (h=15cm) â€¢ Liegend (h=8cm)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Zustandsautomat (State Machine)

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  NORMAL  â”‚ (h = 30cm)
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ Eingang erkannt (h < 25cm)
          â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ ANALYSE  â”‚ (Messe HÃ¶he + Breite)
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â–º h >= 25cm â”€â”€â”€â”€â”€â”€â”€â”€â–º NORMAL
          â”‚
          â”œâ”€â”€â–º 15cm â‰¤ h < 25cm â”€â”€â–º NIEDRIG (Ducken)
          â”‚
          â”œâ”€â”€â–º 10cm â‰¤ h < 15cm â”€â”€â–º LIEGEND (Hinlegen)
          â”‚
          â””â”€â”€â–º h < 10cm â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º BLOCKIERT (Stopp)
```

### Verarbeitungspipeline (Logik-Ablauf)

#### Phase 1: INPUT - Sensing (Messung)
```python
1. Stereo-Depth Camera Capture
   â”œâ”€ Depth Stream (z16) - Tiefeninformation
   â”œâ”€ Color Stream (BGR8) - Visuelle Referenz
   â””â”€ Infrared Stream (Y8) - Low-Light Erkennung

2. LiDAR 2D Scan Capture
   â””â”€ 360Â° Distance Measurements (BodenhÃ¶he)

3. Robot State
   â”œâ”€ Current Body Height (IMU)
   â”œâ”€ Current Posture Mode
   â””â”€ Position & Orientation
```

#### Phase 2: PROCESSING - Black Box (Berechnung)
```python
4. Eingangs-Geometrie Extraktion
   â”œâ”€ HÃ¶henmessung (Unterkante Hindernis â†’ Boden)
   â”œâ”€ Breitenmessung (Links â†’ Rechts)
   â”œâ”€ Neigungserkennung (SchrÃ¤ge Decken)
   â””â”€ Distanzbestimmung (Roboter â†’ Eingang)

5. Sicherheitsberechnung
   â”œâ”€ Ziel_HÃ¶he = Eingangs_HÃ¶he - Sicherheitsabstand
   â”œâ”€ Sicherheitsabstand = 5-10cm (konfigurierbar)
   â””â”€ MinimalhÃ¶he = max(Ziel_HÃ¶he, 8cm)

6. Machbarkeits-Check
   â”œâ”€ Breite >= Roboter_Breite + 5cm ?
   â”œâ”€ HÃ¶he >= 8cm (Minimale liegende Position) ?
   â””â”€ Neigung < 30Â° (StabilitÃ¤tslimit) ?
```

#### Phase 3: DECISION - Entscheidung (State Machine)
```python
7. Haltungs-Entscheidung
   if Ziel_HÃ¶he >= 25cm:
       â†’ NORMAL (Keine Anpassung, h=30cm)
   elif 15cm <= Ziel_HÃ¶he < 25cm:
       â†’ NIEDRIG (Ducken, h=15-25cm)
   elif 10cm <= Ziel_HÃ¶he < 15cm:
       â†’ LIEGEND (Hinlegen, h=8-10cm)
   else:
       â†’ BLOCKIERT (Zu eng, Stopp)

8. Safety Override
   â”œâ”€ Emergency Stop aktiv? â†’ STOPP
   â”œâ”€ Sensorfehler? â†’ SICHERER ZUSTAND
   â””â”€ InstabilitÃ¤t (IMU)? â†’ ABBRUCH
```

#### Phase 4: OUTPUT - Actuation (Ansteuerung)
```python
9. Unitree SDK Kommandos
   â”œâ”€ SetBodyHeight(target_height)
   â”œâ”€ SetPostureMode(mode)  # STAND/CROUCH/PRONE
   â””â”€ SetMovementSpeed(reduced_speed)

10. Bewegungsablauf
    â”œâ”€ Langsames AnnÃ¤hern (0.2 m/s)
    â”œâ”€ Stopp vor Eingang (0.5m Abstand)
    â”œâ”€ HÃ¶henÃ¤nderung (2-3 Sekunden)
    â”œâ”€ Durchquerung (langsam, 0.1 m/s)
    â””â”€ ZurÃ¼ck zu Normal (nach Durchgang)
```

#### Phase 5: MONITORING - Ãœberwachung
```python
11. Kontinuierliches Tracking
    â”œâ”€ Kollisionserkennung (Depth-Daten)
    â”œâ”€ StabilitÃ¤ts-Check (IMU)
    â””â”€ Erfolgs-Validierung (Ist durchgekommen?)

12. Logging & Telemetrie
    â”œâ”€ Eingangs-Dimensionen
    â”œâ”€ GewÃ¤hlter Modus
    â”œâ”€ Durchquerungszeit
    â””â”€ Fehler/Warnungen
```

---

## ðŸ§® Algorithmen und Methoden

### 1. Tiefensprung-Detektion
**Zweck**: Identifikation von Kanten/ÃœbergÃ¤ngen

```python
# Sobel-Gradientenberechnung
Gx = Sobel(depth_image, direction='x')
Gy = Sobel(depth_image, direction='y')
Magnitude = sqrt(GxÂ² + GyÂ²)

# Schwellwert fÃ¼r signifikante SprÃ¼nge
threshold = 30  # anpassbar
discontinuities = Magnitude > threshold
```

**Parameter:**
- Kernel-GrÃ¶ÃŸe: 5x5
- Threshold: 30 (normalisiert 0-255)
- Morphologische Operationen: Close â†’ Open

### 2. Vertikale Strukturerkennung
**Zweck**: TÃ¼rrahmen-Identifikation

```python
# Hough-Transform fÃ¼r Linien
lines = HoughLinesP(
    edges,
    rho=1,              # 1 Pixel AuflÃ¶sung
    theta=Ï€/180,        # 1Â° WinkelauflÃ¶sung
    threshold=50,       # Mindest-Votes
    minLineLength=100,  # MindestlÃ¤nge in Pixel
    maxLineGap=20       # Max. LÃ¼ckengrÃ¶ÃŸe
)

# VertikalitÃ¤ts-Filter
for line in lines:
    angle = atan2(y2-y1, x2-x1) * 180/Ï€
    if 85Â° < angle < 95Â°:  # Â±5Â° Toleranz
        vertical_lines.append(line)
```

**Kriterien:**
- Winkel: 85Â° - 95Â° (vertikal Â±5Â°)
- MindestlÃ¤nge: 100 Pixel (~30cm bei 2m Distanz)
- Max. LÃ¼cke: 20 Pixel

### 3. 3D-Projektion
**Zweck**: Pixel â†’ Weltkoordinaten

```python
# RealSense Intrinsics
fx, fy = intrinsics.fx, intrinsics.fy  # Focal Length
cx, cy = intrinsics.ppx, intrinsics.ppy  # Principal Point

# Deprojektion
X = (x - cx) * depth / fx
Y = (y - cy) * depth / fy
Z = depth

point_3d = [X, Y, Z]  # in Metern
```

**Koordinatensystem:**
- X: Rechts (horizontal)
- Y: Unten (vertikal)
- Z: VorwÃ¤rts (Tiefe)

### 4. Dimensionsmessung
**Zweck**: Breite/HÃ¶he in Metern

```python
# 3D-Distanz zwischen Punkten
left_3d = deproject(x_left, y_center, depth)
right_3d = deproject(x_right, y_center, depth)

width = norm(left_3d - right_3d)

top_3d = deproject(x_center, y_top, depth)
bottom_3d = deproject(x_center, y_bottom, depth)

height = norm(top_3d - bottom_3d)
```

### 5. LiDAR-Gap-Detektion
**Zweck**: DurchgÃ¤nge im 2D-Scan finden

```python
# Nachbarpunkt-Analyse
for i in range(len(points) - 1):
    p1 = points[i]
    p2 = points[i+1]
    
    # Kartesische Distanz
    gap_width = sqrt((p2.x - p1.x)Â² + (p2.y - p1.y)Â²)
    
    # Winkelsprung prÃ¼fen
    angle_diff = abs(p2.angle - p1.angle)
    
    if min_width < gap_width < max_width and angle_diff < 30Â°:
        gaps.append(Gap(p1, p2, gap_width))
```

**Parameter:**
- Min. Breite: 0,7m
- Max. Breite: 2,5m
- Max. Winkelsprung: 30Â°

### 6. Sensor-Fusion
**Zweck**: Kamera + LiDAR kombinieren

```python
# Matching-Score
angle_diff = abs(camera_angle - lidar_angle)
distance_diff = abs(camera_dist - lidar_dist)

angle_score = max(0, 1 - angle_diff/30Â°)
dist_score = max(0, 1 - distance_diff/0.5m)

match_score = (angle_score + dist_score) / 2

# Kombinierte Konfidenz
final_confidence = camera_conf * 0.6 + match_score * 0.4
```

**Gewichtung:**
- Kamera: 60% (primÃ¤re Detektion)
- LiDAR: 40% (Validierung)

### 7. Konfidenz-Berechnung
**Zweck**: QualitÃ¤tsbewertung der Detektion

```python
confidence = 0.0

# Breitenkriteria (40%)
if min_width <= width <= max_width:
    confidence += 0.4

# HÃ¶henkriteria (30%)
if height >= min_height:
    confidence += 0.3

# Distanzkriteria (20%)
if valid_range_min < distance < valid_range_max:
    confidence += 0.2

# Aspect Ratio (10%)
ratio = height / width
if 1.5 < ratio < 4.0:
    confidence += 0.1

# Akzeptanzschwelle
is_valid = confidence > 0.5
```

---

## ðŸ“Š Parameter-Referenz

### Roboter-Parameter (Unitree GO2)

| Parameter | Wert | Einheit | Beschreibung |
|-----------|------|---------|--------------|
| `robot_width` | 0.28 | m | Breite des Roboters |
| `robot_length` | 0.65 | m | LÃ¤nge des Roboters |
| `body_height_normal` | 0.30 | m | Normale StandhÃ¶he |
| `body_height_low` | 0.15 | m | Geduckte Position |
| `body_height_prone` | 0.08 | m | Liegende Position |
| `body_height_min` | 0.08 | m | Absolute MinimalhÃ¶he |

### Erkennungsparameter

| Parameter | Wert | Einheit | Beschreibung |
|-----------|------|---------|--------------|
| `min_entrance_width` | 0.35 | m | Minimale Durchgangsbreite |
| `min_entrance_height` | 0.10 | m | Minimale erkennbare HÃ¶he |
| `max_entrance_height` | 1.00 | m | Maximale relevante HÃ¶he |
| `max_detection_range` | 3.0 | m | Maximale Erkennungsdistanz |
| `safety_clearance` | 0.05-0.10 | m | Sicherheitsabstand oben |
| `confidence_threshold` | 0.6 | - | Min. Konfidenz fÃ¼r Aktion |

### Schwellwerte fÃ¼r Haltungswechsel

| Zustand | HÃ¶henbereich | Aktion |
|---------|--------------|--------|
| **NORMAL** | h â‰¥ 0.25m | Keine Anpassung, weiterfahren |
| **NIEDRIG** | 0.15m â‰¤ h < 0.25m | Ducken, KÃ¶rper senken |
| **LIEGEND** | 0.10m â‰¤ h < 0.15m | Hinlegen, maximale Senkung |
| **BLOCKIERT** | h < 0.10m | Stopp, zu eng |

### Sicherheitsparameter

| Parameter | Wert | Einheit | Beschreibung |
|-----------|------|---------|--------------|
| `approach_distance` | 0.5 | m | Stopp-Abstand vor Eingang |
| `approach_speed` | 0.2 | m/s | AnnÃ¤herungsgeschwindigkeit |
| `traverse_speed` | 0.1 | m/s | Durchquerungsgeschwindigkeit |
| `height_adjust_time` | 2.0-3.0 | s | Zeit fÃ¼r HÃ¶henÃ¤nderung |
| `max_tilt_angle` | 30 | Â° | Max. Neigung/SchrÃ¤ge |
| `emergency_stop_time` | 0.5 | s | Reaktionszeit Notaus |

### Bildverarbeitungsparameter

| Parameter | Wert | Beschreibung |
|-----------|------|--------------|
| `canny_low` | 50 | Unterer Canny-Schwellwert |
| `canny_high` | 150 | Oberer Canny-Schwellwert |
| `hough_threshold` | 50 | Min. Votes fÃ¼r Linie |
| `hough_minLineLength` | 100 | Min. LinienlÃ¤nge (Pixel) |
| `hough_maxLineGap` | 20 | Max. LÃ¼cke in Linie (Pixel) |
| `morphology_kernel` | 5Ã—5 | Kernel fÃ¼r Morphologie |
| `gradient_threshold` | 30 | Schwelle Tiefensprung |

### Filter-Parameter (RealSense)

| Filter | Parameter | Wert |
|--------|-----------|------|
| Spatial | Magnitude | 2 |
| Spatial | Alpha | 0.5 |
| Spatial | Delta | 20 |
| Temporal | Alpha | 0.4 |
| Temporal | Delta | 20 |
| Hole Filling | Mode | Farest from around |

### LiDAR-Parameter

| Parameter | Wert | Einheit | Beschreibung |
|-----------|------|---------|--------------|
| `min_gap_width` | 0.7 | m | Min. LÃ¼ckenbreite |
| `max_gap_width` | 2.5 | m | Max. LÃ¼ckenbreite |
| `max_lidar_range` | 6.0 | m | Max. Messreichweite |
| `angle_tolerance` | 30 | Â° | Max. Winkelsprung |
| `camera_fov` | 69 | Â° | Kamera-Sichtfeld (horizontal) |

### Fusion-Parameter

| Parameter | Wert | Beschreibung |
|-----------|------|--------------|
| `camera_weight` | 0.6 | Gewichtung Kamera-Detektion |
| `lidar_weight` | 0.4 | Gewichtung LiDAR-Validierung |
| `angle_tolerance` | 30Â° | Max. Winkelabweichung |
| `distance_tolerance` | 0.5m | Max. Distanzabweichung |
| `match_threshold` | 0.5 | Min. Score fÃ¼r Match |

---

## ðŸ“ Projektstruktur

```
Bewegungserweiterung/
â”œâ”€â”€ README.md                          # Hauptdokumentation
â”œâ”€â”€ PROJECT_SPEC.md                    # Diese Datei
â”œâ”€â”€ Setup.py                           # Setup-Skript
â”œâ”€â”€ setup.txt                          # Installationsanleitung
â”‚
â”œâ”€â”€ entrance_detection.py              # Basis-Eingangserkennung
â”œâ”€â”€ advanced_entrance_detection.py    # Erweiterte Erkennung + 3D
â”œâ”€â”€ lidar_integration_example.py      # LiDAR-Integration
â”‚
â”œâ”€â”€ opencv_viewer_example.py          # RealSense Viewer Beispiel
â”œâ”€â”€ python-tutorial-1-depth.py        # Depth-Tutorial
â”œâ”€â”€ python-rs400-advanced-mode-example.py
â”œâ”€â”€ rs_launch.py                      # RealSense Launcher
â”‚
â”œâ”€â”€ tests/                            # Unit Tests (geplant)
â”‚   â”œâ”€â”€ test_entrance_detection.py
â”‚   â”œâ”€â”€ test_lidar_processing.py
â”‚   â””â”€â”€ test_sensor_fusion.py
â”‚
â”œâ”€â”€ data/                             # Testdaten
â”‚   â”œâ”€â”€ recordings/                   # RealSense Recordings
â”‚   â””â”€â”€ lidar_scans/                  # LiDAR Scan-Daten
â”‚
â”œâ”€â”€ docs/                             # ZusÃ¤tzliche Dokumentation
â”‚   â”œâ”€â”€ api_reference.md
â”‚   â”œâ”€â”€ calibration_guide.md
â”‚   â””â”€â”€ troubleshooting.md
â”‚
â””â”€â”€ results/                          # Ergebnisse/Logs
    â”œâ”€â”€ visualizations/
    â”œâ”€â”€ performance_logs/
    â””â”€â”€ detection_results/
```

---

## ðŸ”Œ API-Referenz

### EntranceDetector (Basis)

```python
detector = EntranceDetector()

# Hauptmethode
entrances, mask, lines = detector.detect_entrances(depth_frame, color_frame)

# Parameter anpassen
detector.min_entrance_width = 0.8
detector.max_entrance_width = 2.0
detector.depth_threshold = 3.0

# Visualisierung
output = detector.visualize(color_image, entrances, mask, lines)

# Hauptschleife
detector.run()
```

### MultiSensorEntranceDetector (Erweitert)

```python
detector = MultiSensorEntranceDetector(use_lidar=False)

# Frame verarbeiten
color_img, depth_img, entrances = detector.process_frame()

# Entrance-Objekt
entrance = entrances[0]
entrance.position_2d      # (x, y) in Pixeln
entrance.position_3d      # (X, Y, Z) in Metern
entrance.width            # Breite in Metern
entrance.depth            # Entfernung in Metern
entrance.confidence       # 0.0 - 1.0
entrance.is_passable      # Boolean

# LiDAR-Integration
detector.integrate_lidar_data(scan_data)

# Visualisierung
output = detector.visualize(color_image, entrances)
```

### LidarProcessor

```python
lidar = LidarProcessor()

# Scan verarbeiten
lidar.process_scan(scan_data)  # [(angle, distance), ...]

# LÃ¼cken im Kamera-FOV
gaps = lidar.get_gaps_in_camera_fov(camera_fov_deg=69)

# Gap-Objekt
gap = gaps[0]
gap['width']        # Breite in Metern
gap['center']       # (x, y) Position
gap['distance']     # Entfernung
gap['angle']        # Winkel in Grad

# Visualisierung
lidar.visualize_scan()
```

### FusedEntranceDetector

```python
camera_detector = MultiSensorEntranceDetector()
lidar_processor = LidarProcessor()
fused = FusedEntranceDetector(camera_detector, lidar_processor)

# Fusionierte Ergebnisse
results = fused.fuse_detections(camera_entrances, lidar_gaps)

# Result-Objekt
result = results[0]
result['entrance']           # Entrance-Objekt
result['lidar_match']        # Matched Gap
result['confidence']         # Fusionierte Konfidenz
result['confirmed_by_lidar'] # Boolean
```

---

## ðŸ§ª Testing & Validierung

### Test-Szenarien

1. **Einzelner Eingang**
   - Frontale Ansicht
   - Verschiedene Distanzen (0.5m - 5m)
   - Verschiedene Breiten (0.7m - 2.5m)

2. **Multiple EingÃ¤nge**
   - Mehrere TÃ¼ren im Sichtfeld
   - Ãœberlappende Detektionen
   - Priorisierung nach Konfidenz

3. **Herausforderungen**
   - Schlechte Beleuchtung
   - Reflektierende OberflÃ¤chen
   - Transparente TÃ¼ren (Glas)
   - Offene vs. geschlossene TÃ¼ren
   - Teilweise verdeckte EingÃ¤nge

4. **KantenfÃ¤lle**
   - Sehr breite Ã–ffnungen (>2.5m)
   - Sehr schmale DurchgÃ¤nge (<0.7m)
   - Niedrige Decken
   - Stufen/Rampen im Eingang

### Performance-Metriken

| Metrik | Zielwert | Aktuell | Status |
|--------|----------|---------|--------|
| Erkennungsrate | >90% | TBD | ðŸ”„ Testing |
| Falsch-Positiv-Rate | <5% | TBD | ðŸ”„ Testing |
| Verarbeitungszeit | <100ms | TBD | ðŸ”„ Testing |
| FPS | >10 | TBD | ðŸ”„ Testing |
| Positionsgenauigkeit | <10cm | TBD | ðŸ”„ Testing |
| Breitengenauigkeit | <5cm | TBD | ðŸ”„ Testing |

### Benchmarking

```python
import time

# Performance-Test
times = []
for i in range(100):
    start = time.time()
    color_img, depth_img, entrances = detector.process_frame()
    elapsed = time.time() - start
    times.append(elapsed)

print(f"Durchschnitt: {np.mean(times)*1000:.2f}ms")
print(f"FPS: {1/np.mean(times):.1f}")
print(f"Min: {np.min(times)*1000:.2f}ms")
print(f"Max: {np.max(times)*1000:.2f}ms")
```

---

## ðŸš€ Deployment

### Produktiv-Einstellungen

```python
# Optimierte Parameter fÃ¼r Echtzeit
detector = MultiSensorEntranceDetector(use_lidar=True)

# Reduzierte AuflÃ¶sung fÃ¼r Performance
config.enable_stream(rs.stream.depth, 424, 240, rs.format.z16, 30)
config.enable_stream(rs.stream.color, 424, 240, rs.format.bgr8, 30)

# Aggressive Filterung
detector.spatial_filter.set_option(rs.option.filter_magnitude, 3)
detector.temporal_filter.set_option(rs.option.filter_smooth_alpha, 0.6)

# Engere Validierung
detector.confidence_threshold = 0.7
detector.max_detection_range = 3.0
```

### ROS-Integration (geplant)

```python
# ROS Node fÃ¼r Eingangserkennung
import rospy
from geometry_msgs.msg import PoseStamped
from sensor_msgs.msg import Image

class EntranceDetectionNode:
    def __init__(self):
        rospy.init_node('entrance_detection')
        self.pub = rospy.Publisher('/entrances', PoseStamped, queue_size=10)
        self.detector = MultiSensorEntranceDetector()
    
    def publish_entrance(self, entrance):
        pose = PoseStamped()
        pose.pose.position.x = entrance.position_3d[0]
        pose.pose.position.y = entrance.position_3d[1]
        pose.pose.position.z = entrance.position_3d[2]
        self.pub.publish(pose)
```

---

## ðŸ“ˆ Roadmap

### Phase 1: GrundfunktionalitÃ¤t âœ…
- [x] RealSense Integration
- [x] Basis-Eingangserkennung
- [x] 3D-Projektion
- [x] Visualisierung

### Phase 2: Erweiterte Features ðŸ”„
- [x] Tiefenfilterung
- [x] Vertikale Strukturerkennung
- [x] Konfidenz-Scoring
- [x] LiDAR-Integration (Basis)
- [ ] Performance-Optimierung

### Phase 3: Validierung & Testing ðŸ“‹
- [ ] Test-Suite erstellen
- [ ] Benchmark-Szenarien
- [ ] Performance-Messung
- [ ] Kalibrierungs-Tools

### Phase 4: Produktion ðŸŽ¯
- [ ] ROS-Integration
- [ ] SLAM-Integration
- [ ] Pfadplanung
- [ ] Echtzeitnavigation
- [ ] Dokumentation vervollstÃ¤ndigen

---

## ðŸ‘¥ Team & Kontakt

**Projekt**: Bewegungserweiterung (MPEC)  
**Repository**: github.com/eliasbuergin/Bewegungserweiterung  
**Entwickler**: Elias BÃ¼rgin  
**Institution**: [Ihre Institution]  

---

## ðŸ“š Referenzen & Ressourcen

### Dokumentation
- [Intel RealSense SDK](https://dev.intelrealsense.com/)
- [OpenCV Documentation](https://docs.opencv.org/)
- [NumPy Documentation](https://numpy.org/doc/)

### Paper & Forschung
- "Depth-Based Object Detection and Tracking" (2019)
- "Multi-Sensor Fusion for Indoor Navigation" (2020)
- "Real-Time Entrance Detection for Autonomous Robots" (2021)

### Tools & Libraries
- pyrealsense2: RealSense Python Wrapper
- OpenCV: Computer Vision
- NumPy: Numerische Berechnungen
- Matplotlib: Visualisierung
- RPLidar: LiDAR Interface

---

## ðŸ“ Changelog

### Version 0.3.0 (2025-12-01)
- LiDAR-Integration hinzugefÃ¼gt
- Sensor-Fusion implementiert
- Projektdokumentation erstellt

### Version 0.2.0 (2025-11-XX)
- 3D-Projektion implementiert
- Erweiterte Filterung
- Konfidenz-Scoring

### Version 0.1.0 (2025-11-XX)
- Initiale Version
- Basis-Eingangserkennung
- RealSense Integration

---

**Letzte Aktualisierung**: 1. Dezember 2025  
**Status**: In Entwicklung  
**Version**: 0.3.0
